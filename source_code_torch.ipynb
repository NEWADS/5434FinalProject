{
 "cells": [
  {
   "source": [
    "## Below contains necessary APIs for this homework\n",
    "### _Please note that all prequisite libraries can be found in requirements.txt_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# below contains necessary APIs for training.\n",
    "def create_logger(log_path):\n",
    "    \"\"\"\n",
    "    将日志输出到日志文件和控制台\n",
    "    \"\"\"\n",
    "    x = logging.getLogger(__name__)\n",
    "    x.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # 创建一个handler，用于写入日志文件\n",
    "    file_handler = logging.FileHandler(\n",
    "        filename=log_path)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    x.addHandler(file_handler)\n",
    "\n",
    "    # 创建一个handler，用于将日志输出到控制台\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.DEBUG)\n",
    "    console.setFormatter(formatter)\n",
    "    x.addHandler(console)\n",
    "    return x\n",
    "\n",
    "\n",
    "def set_seed(digit: int):\n",
    "    assert digit, 'Please specify a non-zero number when calling this func.'\n",
    "    torch.manual_seed(digit)\n",
    "    np.random.seed(digit)\n",
    "\n",
    "\n",
    "def accuracy(x: np.ndarray, y: np.ndarray):\n",
    "    # calculate accuracy for each class and macro F1-score.\n",
    "    # designed for this task only.\n",
    "    # return correct number of samples corresponds to each class and the number of samples for each class.\n",
    "    # for the overall mean accuracy, just sum them all and divide.\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    index_2 = np.where(y == 2)[0]\n",
    "    x_0, y_0 = x[index_0], y[index_0]\n",
    "    x_1, y_1 = x[index_1], y[index_1]\n",
    "    x_2, y_2 = x[index_2], y[index_2]\n",
    "    acc_0 = 0 if not len(y_0) or not len(x_0) else accuracy_score(y_0, x_0[..., 0], normalize=False)\n",
    "    acc_1 = 0 if not len(y_1) or not len(x_1) else accuracy_score(y_1, x_1[..., 0], normalize=False)\n",
    "    acc_2 = 0 if not len(y_2) or not len(x_2) else accuracy_score(y_2, x_2[..., 0], normalize=False)\n",
    "    return [acc_0, acc_1, acc_2], [len(index_0), len(index_1), len(index_2)]\n",
    "\n",
    "\n",
    "def eval_net(model, loader, device):\n",
    "    model.eval()\n",
    "    n_val = len(loader)  # the number of batch\n",
    "    total_acc_0 = 0\n",
    "    len_0 = 0\n",
    "    total_acc_1 = 0\n",
    "    len_1 = 0\n",
    "    total_acc_2 = 0\n",
    "    len_2 = 0\n",
    "\n",
    "    with tqdm(total=n_val, desc='Evaluation round', unit='batch', leave=False) as pbar:\n",
    "        for bt in loader:\n",
    "            xs, ys = bt['feature'], bt['label']\n",
    "            if isinstance(model, GRU):\n",
    "                xs = xs.long()\n",
    "            xs = xs.to(device=device)\n",
    "            ys = ys.to(device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if isinstance(model, ANN) or isinstance(model, GRU):\n",
    "                    xs = torch.squeeze(xs)\n",
    "                refs = model(xs)\n",
    "                refs = F.softmax(refs, dim=1)\n",
    "                refs = torch.argmax(refs, dim=1, keepdim=True)\n",
    "            accuracies, lens = accuracy(refs.detach().cpu().numpy(), ys.detach().cpu().numpy())\n",
    "            total_acc_0 += accuracies[0]\n",
    "            len_0 += lens[0]\n",
    "            total_acc_1 += accuracies[1]\n",
    "            len_1 += lens[1]\n",
    "            total_acc_2 += accuracies[2]\n",
    "            len_2 += lens[2]\n",
    "            pbar.update()\n",
    "\n",
    "    model.train()\n",
    "    return total_acc_0 / len_0, total_acc_1 / len_1, total_acc_2 / len_2\n",
    "\n",
    "\n",
    "# below is our dataset function.\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, x: str, shuffle: bool = True, seed: int = 1, balanced=True, delta_1=1, delta_2=1):\n",
    "        super(SequenceDataset, self).__init__()\n",
    "        raw_data = np.load(x)\n",
    "        self.features = raw_data['reads'].astype(np.float32)\n",
    "        try:\n",
    "            self.labels = raw_data['label']\n",
    "        except KeyError:\n",
    "            self.labels = np.ones(self.features.shape[0]) * -1\n",
    "        self.labels = self.labels.astype(np.int64)\n",
    "        self.ids = np.arange(len(self.labels)).astype(np.int64)\n",
    "        if shuffle:\n",
    "            self.features, self.labels, self.ids = sklearn.utils.shuffle(self.features,\n",
    "                                                                         self.labels,\n",
    "                                                                         self.ids, random_state=seed)\n",
    "        if balanced:\n",
    "            x_train_0, y_train_0 = self.features[np.where(self.labels == 0)[0]], self.labels[np.where(self.labels == 0)[0]]\n",
    "            ids_0 = self.ids[np.where(self.labels == 0)[0]]\n",
    "            x_train_1, y_train_1 = self.features[np.where(self.labels == 1)[0]], self.labels[np.where(self.labels == 1)[0]]\n",
    "            ids_1 = self.ids[np.where(self.labels == 1)[0]]\n",
    "            # randomly sample some data to make the dataset looks more equally distributed.\n",
    "            x_train_1, y_train_1, ids_1 = x_train_1[:int(19713 * delta_1), :], y_train_1[:int(19713 * delta_1)], ids_1[:int(19713 * delta_1)]\n",
    "            x_train_2, y_train_2 = self.features[np.where(self.labels == 2)[0]], self.labels[np.where(self.labels == 2)[0]]\n",
    "            ids_2 = self.ids[np.where(self.labels == 2)[0]]\n",
    "            # randomly sample some data to make the dataset looks more equally distributed.\n",
    "            x_train_2, y_train_2, ids_2 = x_train_2[:int(19713 * delta_2), :], y_train_2[:int(19713 * delta_2)], ids_2[:int(19713 * delta_2)]\n",
    "            self.features = np.concatenate([x_train_0, x_train_1, x_train_2], axis=0)\n",
    "            self.labels = np.concatenate([y_train_0, y_train_1, y_train_2], axis=0)\n",
    "            self.ids = np.concatenate([ids_0, ids_1, ids_2], axis=0)\n",
    "            self.features, self.labels, self.ids = sklearn.utils.shuffle(self.features,\n",
    "                                                                         self.labels,\n",
    "                                                                         self.ids, random_state=seed)\n",
    "        self.seed = seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label, index = self.features[idx], self.labels[idx], self.ids[idx]\n",
    "        # feature = [_encode(i) for i in feature]\n",
    "        return {'feature': torch.tensor(feature, dtype=torch.float32),\n",
    "                'label': torch.tensor(label, dtype=torch.int64),\n",
    "                'index': torch.tensor(index, dtype=torch.int64)}\n",
    "\n",
    "\n",
    "# Below contains all implemented nn models for this task.\n",
    "class ANN(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: list, n_class: int = 3, drop_rate=0.2):\n",
    "        super(ANN, self).__init__()\n",
    "        linears = [nn.Linear(250, layers[0]), nn.Tanh(), nn.Dropout(p=drop_rate)]\n",
    "        # pay attention to here\n",
    "        for i in range(len(layers) - 1):\n",
    "            linears.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            linears.append(nn.Tanh())\n",
    "            linears.append(nn.Dropout(p=drop_rate))\n",
    "        self.features = nn.Sequential(*linears)\n",
    "        self.out = nn.Linear(layers[-1], n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        logits = self.out(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class IdentityBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_sample=False, kernel_size=3, padding=1, **kwargs):\n",
    "        super(IdentityBlock1D, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        stride = 2 if down_sample else 1\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        # The first conv layer has to follow the original paper so as to make sure feature map downsampled correctly.\n",
    "        self.norm1 = nn.BatchNorm1d(out_channels)\n",
    "        self.non_linear1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.norm2 = nn.BatchNorm1d(out_channels)\n",
    "        self.non_linear2 = nn.ReLU()\n",
    "        if down_sample:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, stride=2),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # obviously, up-conv layer must be applied before identity block in decoder block.\n",
    "        if self.down_sample:\n",
    "            identity = self.residual(x)\n",
    "        else:\n",
    "            identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.non_linear1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.non_linear2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def _make_stage(self, in_c, out_c, down_s=True, num_l=2):\n",
    "        stage_list = [IdentityBlock1D(in_c, out_c, down_s)]\n",
    "        for _ in range(1, num_l):\n",
    "            stage_list.append(IdentityBlock1D(out_c, out_c))\n",
    "        return nn.Sequential(*stage_list)\n",
    "\n",
    "    def _configure_layers(self, stage=18):\n",
    "        if stage == 18:\n",
    "            return [2, 2, 2, 2]\n",
    "        else:\n",
    "            return [3, 4, 6, 3]\n",
    "\n",
    "    def __init__(self, in_channels: int, layers: list, n_class: int = 3, stages: int = 18):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=layers[0], kernel_size=7, stride=2, padding=3)\n",
    "        self.norm1 = nn.BatchNorm1d(layers[0])\n",
    "        self.non_linear1 = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        config = self._configure_layers(stages)\n",
    "        self.stage1 = self._make_stage(layers[0], layers[0], down_s=False, num_l=config[0])\n",
    "        self.stage2 = self._make_stage(layers[0], layers[1], down_s=True, num_l=config[1])\n",
    "        self.stage3 = self._make_stage(layers[1], layers[2], down_s=True, num_l=config[2])\n",
    "        self.stage4 = self._make_stage(layers[2], layers[3], down_s=True, num_l=config[3])\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(layers[3], n_class)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.non_linear1(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet1D_M(nn.Module):\n",
    "    def _make_stage(self, in_c, out_c, down_s=True, num_l=2, ks=3, pa=1):\n",
    "        stage_list = [IdentityBlock1D(in_c, out_c, down_s, ks, pa)]\n",
    "        for _ in range(1, num_l):\n",
    "            stage_list.append(IdentityBlock1D(out_c, out_c, kernel_size=ks, padding=pa))\n",
    "        return nn.Sequential(*stage_list)\n",
    "\n",
    "    def _configure_layers(self, stage=18):\n",
    "        if stage == 18:\n",
    "            return [2, 2, 2, 2]\n",
    "        else:\n",
    "            return [3, 4, 6, 3]\n",
    "\n",
    "    def __init__(self, in_channels: int, layers: list, n_class: int = 3, stages: int = 18, kernel_size=3, padding=1,\n",
    "                 drop_rate=0.0):\n",
    "        super(ResNet1D_M, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=layers[0], kernel_size=7, stride=1, padding=3)\n",
    "        self.norm1 = nn.BatchNorm1d(layers[0])\n",
    "        self.non_linear1 = nn.ReLU()\n",
    "        # self.max_pool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        config = self._configure_layers(stages)\n",
    "        self.stage1 = self._make_stage(layers[0], layers[0], down_s=False, num_l=config[0], ks=kernel_size, pa=padding)\n",
    "        self.stage2 = self._make_stage(layers[0], layers[1], down_s=True, num_l=config[1], ks=kernel_size, pa=padding)\n",
    "        self.stage3 = self._make_stage(layers[1], layers[2], down_s=True, num_l=config[2], ks=kernel_size, pa=padding)\n",
    "        self.stage4 = self._make_stage(layers[2], layers[3], down_s=True, num_l=config[3], ks=kernel_size, pa=padding)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(layers[3], n_class)\n",
    "        if drop_rate:\n",
    "            self.drop = nn.Dropout(p=drop_rate)\n",
    "        else:\n",
    "            self.drop = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.non_linear1(x)\n",
    "        # x = self.max_pool(x)\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        if isinstance(self.drop, nn.Dropout):\n",
    "            x = self.drop(x)\n",
    "        x = self.stage2(x)\n",
    "        if isinstance(self.drop, nn.Dropout):\n",
    "            x = self.drop(x)\n",
    "        x = self.stage3(x)\n",
    "        if isinstance(self.drop, nn.Dropout):\n",
    "            x = self.drop(x)\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        if isinstance(self.drop, nn.Dropout):\n",
    "            x = self.drop(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    # please note that GRU does not accept one_hot data.\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, num_layers: int = 2, n_class: int = 3, drop_rate=0.2):\n",
    "        # embedding_dim refers to the dimension of the embedding vector to better encode the input vector.\n",
    "        \"\"\"\n",
    "        Pay attention:\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(4, embedding_dim)  # ATGC\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=drop_rate,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [batch, sequence]\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds : [batch, sequence, embedding_dim]\n",
    "        r_out, _ = self.gru(embeds, None)\n",
    "        # r_out : [batch, sequence, hidden_dim]\n",
    "        # suggestion from poncey.\n",
    "        # out = self.fc(r_out[:, -1, :])\n",
    "        r_out = torch.mean(r_out, dim=1)\n",
    "        out = self.fc(r_out)\n",
    "        # out : [batch, sequence, output_dim]\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Below contains the training function\n",
    "### _Please note that the config recorded in this section only corresponds to GRU's best performance, for the config of other models, please contact wilszhang2-c@my.cityu.edu.hk._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import socket\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "config_dict = dict(\n",
    "    seed=114,\n",
    "    balanced=True,\n",
    "    delta_1=2,\n",
    "    delta_2=2,\n",
    "    class_weight=[2, 1, 1],\n",
    "    model='gru',\n",
    "    # this entry works for ANN and CNN\n",
    "    layers=[16, 32, 64, 128],\n",
    "    # these entries work for CNN only\n",
    "    stages=18,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    # these entries work for GRU only\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    num_layers=3,\n",
    "    # this entry works for all networks\n",
    "    drop_rate=0.2,\n",
    "    epochs=30,\n",
    "    # optimizer\n",
    "    optim='adam',\n",
    "    lr=2e-04,\n",
    "    decay=True,\n",
    "    batch_size=1024,\n",
    "    one_hot=False,\n",
    ")\n",
    "\n",
    "SEED = config_dict['seed']\n",
    "TIME = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "CLASS_WEIGHT = torch.tensor(config_dict['class_weight'], dtype=torch.float)\n",
    "DEVICE = 'cuda:0'\n",
    "BALANCED = config_dict['balanced']\n",
    "# LAYERS = config_dict['layers']\n",
    "EPOCH = config_dict['epochs']\n",
    "LR = config_dict['lr']\n",
    "BATCH_SIZE = config_dict['batch_size']\n",
    "MODEL_DICT = None\n",
    "PARALLEL = False\n",
    "DECAY = config_dict['decay']\n",
    "DELTA_1 = config_dict['delta_1']\n",
    "DELTA_2 = config_dict['delta_2']\n",
    "ONE_HOT = config_dict['one_hot']  # when using GRU and CNN, this should be false.\n",
    "\n",
    "if not ONE_HOT:                                                                                                   \n",
    "    train_data = SequenceDataset(x='./train_expand.npz', shuffle=True, seed=SEED, balanced=BALANCED,              \n",
    "                                 delta_1=DELTA_1, delta_2=DELTA_2)                                                \n",
    "    val_data = SequenceDataset(x='./val_expand.npz', shuffle=False, balanced=False)                               \n",
    "    test_data = SequenceDataset(x='./test_expand.npz', shuffle=False, balanced=False)                             \n",
    "else:                                                                                                             \n",
    "    train_data = SequenceDataset(x='./train_one_hot.npz', shuffle=True, seed=SEED, balanced=BALANCED,             \n",
    "                                 delta_1=DELTA_1, delta_2=DELTA_2)                                                \n",
    "    val_data = SequenceDataset(x='./val_one_hot.npz', shuffle=False, balanced=False)                              \n",
    "    test_data = SequenceDataset(x='./test_one_hot.npz', shuffle=False, balanced=False)                            \n",
    "# create logger and model path.                                                                                   \n",
    "if not os.path.exists('./runs/{}'.format(config_dict['model'])):                                                  \n",
    "    os.makedirs('./runs/{}'.format(config_dict['model']))                                                         \n",
    "if SEED:\n",
    "    set_seed(SEED)\n",
    "logger = create_logger('./runs/{}/{}_{}.log'.format(config_dict['model'], TIME, socket.gethostname()))            \n",
    "if 'cuda' in DEVICE:\n",
    "    if not torch.cuda.is_available():                                                                             \n",
    "        raise ValueError('CUDA specified but not detected.')                                                      \n",
    "    else:                                                                                                         \n",
    "        torch.backends.cudnn.deterministic = True                                                                 \n",
    "        torch.backends.cudnn.benchmark = False                                                                    \n",
    "logger.info('%-40s %s\\n' % ('Using device', DEVICE))                                                              \n",
    "if config_dict['model'] == 'resnet_m':\n",
    "    model = ResNet1D_M(in_channels=4 if ONE_HOT else 1, layers=config_dict['layers'], n_class=3,                  \n",
    "                       stages=config_dict['stages'], kernel_size=config_dict['kernel_size'],                      \n",
    "                       padding=config_dict['padding'], drop_rate=config_dict['drop_rate'])                        \n",
    "elif config_dict['model'] == 'ann':\n",
    "    model = ANN(layers=config_dict['layers'], drop_rate=config_dict['drop_rate'])                                 \n",
    "else:\n",
    "    model = GRU(embedding_dim=config_dict['embedding_dim'], hidden_dim=config_dict['hidden_dim'],                 \n",
    "                num_layers=config_dict['num_layers'], drop_rate=config_dict['drop_rate'])                         \n",
    "if PARALLEL:                                                                                                      \n",
    "    model = nn.DataParallel(model)                                                                                \n",
    "if MODEL_DICT:                                                                                                    \n",
    "    checkpoint = torch.load(MODEL_DICT)                                                                           \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])                                                         \n",
    "model = model.to(device=DEVICE)                                                                                   \n",
    "CLASS_WEIGHT = CLASS_WEIGHT.to(device=DEVICE)                                                                     \n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHT)                                                              \n",
    "# criterion = FocalLoss()                                                                                         \n",
    "# optimizer = Adam(model.parameters(), LR)\n",
    "# if you found the performance is not equivalent to what we report on the paper, try to cancel weight decay.\n",
    "if config_dict['optim'] == 'sgd':                                                                                 \n",
    "    optimizer = SGD(model.parameters(), LR, momentum=0.99, weight_decay=5e-04)                                    \n",
    "else:                                                                                                             \n",
    "    optimizer = Adam(model.parameters(), LR, weight_decay=5e-04)                                                  \n",
    "                                                                                                                  \n",
    "logger.info(' Config '.center(80, '-'))                                                                           \n",
    "name_format = '%-40s %s\\n' * 12                                                                                   \n",
    "logger.info(name_format % (\"Learning Rate\", LR,                                                                   \n",
    "                           \"Decay\", DECAY,                                                                        \n",
    "                           \"Balanced Set\", BALANCED,                                                              \n",
    "                           \"Loss Function\", criterion,                                                            \n",
    "                           \"Class Weight\", CLASS_WEIGHT.clone().detach().cpu().numpy(),                           \n",
    "                           \"Epoch\", EPOCH,                                                                        \n",
    "                           \"Batch Size\", BATCH_SIZE,                                                              \n",
    "                           'Seed', SEED,                                                                          \n",
    "                           \"One-hot feature set\", ONE_HOT,                                                        \n",
    "                           \"Training set size\", train_data.__len__(),                                             \n",
    "                           \"Validation Set Size\", val_data.__len__(),                                             \n",
    "                           \"Test Set Size\", test_data.__len__()))                                                 \n",
    "logger.info(f'\\t{model}')                                                                                         \n",
    "logger.info('-' * 80)                                                                                             \n",
    "if DECAY:                                                                                                         \n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, [cc for cc in range(1, EPOCH, 1)], gamma=0.99)                \n",
    "else:                                                                                                             \n",
    "    scheduler = None                                                                                              \n",
    "                                                                                                                  \n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)  # already shuffled.    \n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)                            \n",
    "                                                                                                                  \n",
    "writer = SummaryWriter(log_dir='./runs/{}/{}_{}/'.format(config_dict['model'], TIME, socket.gethostname()))       \n",
    "global_step = 0                                                                                                   \n",
    "# start training.                                                                                                 \n",
    "for epoch in range(1, EPOCH + 1):                                                                                 \n",
    "    model.train()                                                                                                 \n",
    "    with tqdm(total=train_data.__len__(), desc=f'Epoch {epoch}/{EPOCH}', unit='seqs') as pbar:                    \n",
    "        for batch in train_loader:                                                                                \n",
    "            features = batch['feature'].to(device=DEVICE)                                                         \n",
    "            if isinstance(model, GRU):                                                                            \n",
    "                features = features.long()                                                                        \n",
    "            labels = batch['label'].to(device=DEVICE)                                                             \n",
    "                                                                                                                  \n",
    "            with torch.no_grad():                                                                                 \n",
    "                if isinstance(model, ANN) or isinstance(model, GRU):                                              \n",
    "                    features = torch.squeeze(features)                                                            \n",
    "            pred = model(features)                                                                                \n",
    "            loss = criterion(pred, labels)                                                                        \n",
    "            writer.add_scalar('loss/train_criterion', loss.item(), global_step)                                   \n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})                                                     \n",
    "                                                                                                                  \n",
    "            optimizer.zero_grad()                                                                                 \n",
    "            loss.backward()                                                                                       \n",
    "            nn.utils.clip_grad_value_(model.parameters(), 0.1)                                                    \n",
    "            optimizer.step()                                                                                      \n",
    "                                                                                                                  \n",
    "            pbar.update(features.shape[0])                                                                        \n",
    "            global_step += 1                                                                                      \n",
    "                                                                                                                  \n",
    "        if DECAY:                                                                                                 \n",
    "            scheduler.step()                                                                                      \n",
    "            writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)                            \n",
    "    if not epoch % 4 or epoch == EPOCH:                                                                           \n",
    "        mean_acc_0, mean_acc_1, mean_acc_2 = eval_net(model, val_loader, DEVICE)                                  \n",
    "        logger.info('Validation Accuracy of class 0 for epoch {}: {}'.format(epoch, mean_acc_0))                  \n",
    "        logger.info('Validation Accuracy of class 1 for epoch {}: {}'.format(epoch, mean_acc_1))                  \n",
    "        logger.info('Validation Accuracy of class 2 for epoch {}: {}'.format(epoch, mean_acc_2))                  \n",
    "        writer.add_scalar('val_acc/0', mean_acc_0, epoch)                                                         \n",
    "        writer.add_scalar('val_acc/1', mean_acc_1, epoch)                                                         \n",
    "        writer.add_scalar('val_acc/2', mean_acc_2, epoch)                                                         \n",
    "                                                                                                                  \n",
    "# training finished, start output evaluation results.                                                             \n",
    "logger.info(\"Saving testing set predictions for epoch {}\".format(EPOCH))                                          \n",
    "test_loader = DataLoader(test_data, batch_size=200, shuffle=False)                                                \n",
    "model.eval()                                                                                                      \n",
    "outputs = []                                                                                                      \n",
    "out_ids = []                                                                                                      \n",
    "n_test = len(test_loader)  # the number of batch                                                                  \n",
    "with tqdm(total=n_test, desc='Output Testing result', unit='batch', leave=False) as pbar:                         \n",
    "    for batch in test_loader:                                                                                     \n",
    "        features, ids = batch['feature'].to(device=DEVICE), batch['index'].to(device='cpu')                       \n",
    "        if isinstance(model, GRU):                                                                                \n",
    "            features = features.long()                                                                            \n",
    "                                                                                                                  \n",
    "        with torch.no_grad():                                                                                     \n",
    "            if isinstance(model, ANN) or isinstance(model, GRU):                                                  \n",
    "                features = torch.squeeze(features)                                                                \n",
    "            preds = model(features)                                                                               \n",
    "            preds = F.softmax(preds, dim=1)                                                                       \n",
    "            preds = torch.argmax(preds, dim=1, keepdim=True)                                                      \n",
    "        outputs.append(preds.detach().cpu().numpy())                                                              \n",
    "        out_ids.append(ids.detach().cpu().numpy())                                                                \n",
    "        pbar.update()                                                                                             \n",
    "outputs = np.concatenate(outputs, axis=0)                                                                         \n",
    "out_ids = np.concatenate(out_ids, axis=0)  # debug needed                                                         \n",
    "res = {'ID': out_ids, 'label': outputs[:, 0]}                                                                     \n",
    "df = pd.DataFrame(data=res, dtype=np.int)                                                                         \n",
    "df.to_csv('./runs/{}/{}_{}/{}_epoch_{}.csv'.format(config_dict['model'], TIME, socket.gethostname(),              \n",
    "                                                   model.__class__.__name__, EPOCH),                              \n",
    "          index=False)                                                                                            \n",
    "logger.info(\"Saving model for final epoch {}\".format(EPOCH))                                                      \n",
    "torch.save({                                                                                                      \n",
    "    'epoch': EPOCH,                                                                                               \n",
    "    'model_state_dict': model.state_dict()                                                                        \n",
    "}, './runs/{}/{}_{}/{}_epoch_{}.pt'.format(config_dict['model'], TIME, socket.gethostname(),                      \n",
    "                                           model.__class__.__name__, EPOCH))                                      \n",
    "np.save('./runs/{}/{}_{}/config_dict.npy'.format(config_dict['model'], TIME, socket.gethostname()), config_dict)  \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0fb16b9bb9871b27dceeb93c88e6ebd31ea18f3dc1b6d4771e79af536d9b81f70",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}